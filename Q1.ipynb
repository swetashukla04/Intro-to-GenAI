{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Python Notebook: Vanilla RNN from Scratch (Without Classes)\n",
    "\n",
    "This notebook demonstrates a basic implementation of a Vanilla Recurrent Neural Network (RNN)\n",
    "from scratch using Python and NumPy, without using a class structure. It focuses on the\n",
    "core concepts of RNNs, including forward and backward passes, weight sharing, and\n",
    "gradient updates.\n",
    "\n",
    "Note: This implementation is for educational purposes and may not be optimized for\n",
    "performance or handle all edge cases. For practical applications, consider using\n",
    "established deep learning libraries like TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 1. Initialization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 3\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize weights with small random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
    "W = np.random.randn(hidden_size, hidden_size) * 0.01 # Hidden to hidden\n",
    "V = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize biases with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "c = np.zeros((output_size, 1))  # Output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 2. Forward Pass ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Applies the softmax activation function.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x)) # Subtract max for numerical stability\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, U, W, V, b, c):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of the RNN.\n",
    "\n",
    "    Args:\n",
    "        inputs (list of numpy arrays): A list of input vectors, where each vector\n",
    "                                      has the shape (input_size, 1).\n",
    "        U (numpy array): Input to hidden weights.\n",
    "        W (numpy array): Hidden to hidden weights.\n",
    "        V (numpy array): Hidden to output weights.\n",
    "        b (numpy array): Hidden bias.\n",
    "        c (numpy array): Output bias.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - outputs (list of numpy arrays): The output probabilities at each time step.\n",
    "            - hidden_states (numpy array): The hidden states at each time step (including initial).\n",
    "    \"\"\"\n",
    "    T = len(inputs)\n",
    "    hidden_states = np.zeros((T + 1, hidden_size, 1)) # Store hidden states (including initial)\n",
    "    outputs = []\n",
    "\n",
    "    # Initial hidden state is often set to zero\n",
    "    hidden_states[0] = np.zeros((hidden_size, 1))\n",
    "\n",
    "    for t in range(T):\n",
    "        # Calculate the next hidden state\n",
    "        hidden_states[t+1] = np.tanh(np.dot(U, inputs[t]) + np.dot(W, hidden_states[t]) + b)\n",
    "\n",
    "        # Calculate the output\n",
    "        output = softmax(np.dot(V, hidden_states[t+1]) + c)\n",
    "        outputs.append(output)\n",
    "\n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 3. Backward Pass ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(inputs, outputs, hidden_states, targets, U, W, V, b, c):\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the RNN.\n",
    "\n",
    "    Args:\n",
    "        inputs (list of numpy arrays): The input sequence.\n",
    "        outputs (list of numpy arrays): The output probabilities from the forward pass.\n",
    "        hidden_states (numpy array): The hidden states from the forward pass.\n",
    "        targets (list of int): The target class indices for each time step.\n",
    "        U (numpy array): Input to hidden weights.\n",
    "        W (numpy array): Hidden to hidden weights.\n",
    "        V (numpy array): Hidden to output weights.\n",
    "        b (numpy array): Hidden bias.\n",
    "        c (numpy array): Output bias.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the gradients for the weights and biases:\n",
    "            - dU (numpy array): Gradient of the loss with respect to U.\n",
    "            - dW (numpy array): Gradient of the loss with respect to W.\n",
    "            - dV (numpy array): Gradient of the loss with respect to V.\n",
    "            - db (numpy array): Gradient of the loss with respect to b.\n",
    "            - dc (numpy array): Gradient of the loss with respect to c.\n",
    "    \"\"\"\n",
    "    T = len(inputs)\n",
    "    dU = np.zeros_like(U)\n",
    "    dW = np.zeros_like(W)\n",
    "    dV = np.zeros_like(V)\n",
    "    db = np.zeros_like(b)\n",
    "    dc = np.zeros_like(c)\n",
    "    dh_next = np.zeros_like(hidden_states[0]) # Gradient of loss w.r.t. the next hidden state\n",
    "\n",
    "    # Iterate backwards through time\n",
    "    for t in reversed(range(T)):\n",
    "        # Convert target to a one-hot vector\n",
    "        target = np.zeros_like(outputs[t])\n",
    "        target[targets[t]] = 1\n",
    "\n",
    "        # Output layer gradient\n",
    "        dy = outputs[t] - target\n",
    "        dV += np.dot(dy, hidden_states[t+1].T)\n",
    "        dc += dy\n",
    "\n",
    "        # Gradient of the hidden state\n",
    "        dh = np.dot(V.T, dy) + dh_next\n",
    "\n",
    "        # Gradient of tanh activation\n",
    "        dtanh = (1 - hidden_states[t+1] ** 2) * dh\n",
    "\n",
    "        # Gradients for weights and biases\n",
    "        db += dtanh\n",
    "        dU += np.dot(dtanh, inputs[t].T)\n",
    "        dW += np.dot(dtanh, hidden_states[t].T)\n",
    "\n",
    "        # Update dh_next for the previous time step\n",
    "        dh_next = np.dot(W.T, dtanh)\n",
    "\n",
    "    return dU, dW, dV, db, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 4. Training Step ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, targets, U, W, V, b, c, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs one step of training (forward and backward pass, followed by weight update).\n",
    "\n",
    "    Args:\n",
    "        inputs (list of numpy arrays): The input sequence.\n",
    "        targets (list of int): The target class indices for each time step.\n",
    "        U (numpy array): Input to hidden weights.\n",
    "        W (numpy array): Hidden to hidden weights.\n",
    "        V (numpy array): Hidden to output weights.\n",
    "        b (numpy array): Hidden bias.\n",
    "        c (numpy array): Output bias.\n",
    "        learning_rate (float): The learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the updated weights and biases (U, W, V, b, c) and the average loss.\n",
    "    \"\"\"\n",
    "    outputs, hidden_states = forward(inputs, U, W, V, b, c)\n",
    "    dU, dW, dV, db, dc = backward(inputs, outputs, hidden_states, targets, U, W, V, b, c)\n",
    "\n",
    "    # Update weights and biases\n",
    "    U_new = U - learning_rate * dU\n",
    "    W_new = W - learning_rate * dW\n",
    "    V_new = V - learning_rate * dV\n",
    "    b_new = b - learning_rate * db\n",
    "    c_new = c - learning_rate * dc\n",
    "\n",
    "    # Calculate the loss (e.g., cross-entropy loss)\n",
    "    loss = 0\n",
    "    for t in range(len(targets)):\n",
    "        correct_prob = outputs[t][targets[t], 0]\n",
    "        loss -= np.log(correct_prob + 1e-8) # Add small epsilon for numerical stability\n",
    "    return U_new, W_new, V_new, b_new, c_new, loss / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 5. Example Usage ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Average Loss: 1.0982\n",
      "Epoch 20/100, Average Loss: 1.0971\n",
      "Epoch 30/100, Average Loss: 1.0948\n",
      "Epoch 40/100, Average Loss: 1.0880\n",
      "Epoch 50/100, Average Loss: 1.0696\n",
      "Epoch 60/100, Average Loss: 1.0285\n",
      "Epoch 70/100, Average Loss: 0.9619\n",
      "Epoch 80/100, Average Loss: 0.8846\n",
      "Epoch 90/100, Average Loss: 0.8087\n",
      "Epoch 100/100, Average Loss: 0.7354\n",
      "\n",
      "Test Input:\n",
      "[[ 0.18079297  0.5110085  -1.18064333  0.34018705 -0.41573644]\n",
      " [-0.01421383 -0.04545861 -0.79667118  0.81290003 -0.49295355]\n",
      " [ 0.75182443  1.37439125  0.68229239  0.27691023 -0.93631094]\n",
      " [ 0.20859064 -0.27829881 -0.50461821 -0.79048444 -0.70402334]]\n",
      "Predicted Output Probabilities (for each time step):\n",
      "[0.33013468 0.50250206 0.16736326]\n",
      "[0.42897521 0.31272043 0.25830436]\n",
      "[0.30548786 0.4024478  0.29206434]\n",
      "[0.33512743 0.1553718  0.50950077]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define hyperparameters\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Generate some dummy training data\n",
    "    # Each sequence has a variable length\n",
    "    training_data = [\n",
    "        (np.random.randn(3, input_size, 1), [0, 1, 2]),\n",
    "        (np.random.randn(5, input_size, 1), [1, 2, 0, 1, 2]),\n",
    "        (np.random.randn(2, input_size, 1), [2, 0]),\n",
    "        (np.random.randn(4, input_size, 1), [0, 0, 1, 1]),\n",
    "    ]\n",
    "\n",
    "    # Train the RNN\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in training_data:\n",
    "            U, W, V, b, c, loss = train_step(list(inputs), targets, U, W, V, b, c, learning_rate)\n",
    "            total_loss += loss\n",
    "        avg_loss = total_loss / len(training_data)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Test the trained RNN (simple forward pass)\n",
    "    test_input = np.random.randn(4, input_size, 1)\n",
    "    outputs, _ = forward(list(test_input), U, W, V, b, c)\n",
    "    print(\"\\nTest Input:\")\n",
    "    print(test_input.squeeze())\n",
    "    print(\"Predicted Output Probabilities (for each time step):\")\n",
    "    for output in outputs:\n",
    "        print(output.flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
