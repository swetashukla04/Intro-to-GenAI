{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-Level RNN for Text Generation with PyTorch\n",
    "\n",
    "This notebook demonstrates how to build a Recurrent Neural Network (RNN)\n",
    "using PyTorch to generate text character by character.\n",
    "\n",
    "It covers:\n",
    "- Converting text into numerical representations (one-hot vectors).\n",
    "- Handling sequential dependencies with hidden states.\n",
    "- Training with Teacher Forcing.\n",
    "- Implementing sampling strategies like greedy search and temperature-based sampling.\n",
    "\n",
    "Bonus: Provides a brief outline for extending to word-level generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog. \" * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a set of unique characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: [' ', '.', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocabulary size: 29\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Unique characters: {chars}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create character-to-index and index-to-character mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {char: i for i, char in enumerate(chars)}\n",
    "index_to_char = {i: char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to a sequence of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 225\n",
      "First 10 characters as indices: [2, 10, 7, 0, 19, 23, 11, 5, 13, 0]\n"
     ]
    }
   ],
   "source": [
    "data = [char_to_index[char] for char in text]\n",
    "print(f\"Length of the text: {len(data)}\")\n",
    "print(f\"First 10 characters as indices: {data[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        # hidden shape (for LSTM): (h_n, c_n) where h_n and c_n are (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Embed the input characters\n",
    "        embedded = self.embedding(x) # shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Pass the embedded input through the RNN\n",
    "        out, hidden = self.rnn(embedded, hidden) # out shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Pass the output through the fully connected layer\n",
    "        out = self.fc(out) # shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden and cell states for LSTM\n",
    "        return (torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size).to(device),\n",
    "                torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.3761, Time: 0.01s\n",
      "Epoch [2/100], Loss: 3.2622, Time: 0.01s\n",
      "Epoch [3/100], Loss: 3.0422, Time: 0.03s\n",
      "Epoch [4/100], Loss: 3.1399, Time: 0.04s\n",
      "Epoch [5/100], Loss: 2.8525, Time: 0.04s\n",
      "Epoch [6/100], Loss: 2.7028, Time: 0.05s\n",
      "Epoch [7/100], Loss: 2.5701, Time: 0.05s\n",
      "Epoch [8/100], Loss: 2.3846, Time: 0.06s\n",
      "Epoch [9/100], Loss: 2.1569, Time: 0.06s\n",
      "Epoch [10/100], Loss: 1.9037, Time: 0.07s\n",
      "Epoch [11/100], Loss: 1.6364, Time: 0.07s\n",
      "Epoch [12/100], Loss: 1.3820, Time: 0.08s\n",
      "Epoch [13/100], Loss: 1.1517, Time: 0.09s\n",
      "Epoch [14/100], Loss: 0.9463, Time: 0.09s\n",
      "Epoch [15/100], Loss: 0.7687, Time: 0.09s\n",
      "Epoch [16/100], Loss: 0.6177, Time: 0.10s\n",
      "Epoch [17/100], Loss: 0.4923, Time: 0.10s\n",
      "Epoch [18/100], Loss: 0.3939, Time: 0.11s\n",
      "Epoch [19/100], Loss: 0.3145, Time: 0.11s\n",
      "Epoch [20/100], Loss: 0.2500, Time: 0.11s\n",
      "Epoch [21/100], Loss: 0.1995, Time: 0.12s\n",
      "Epoch [22/100], Loss: 0.1612, Time: 0.12s\n",
      "Epoch [23/100], Loss: 0.1324, Time: 0.12s\n",
      "Epoch [24/100], Loss: 0.1106, Time: 0.13s\n",
      "Epoch [25/100], Loss: 0.0942, Time: 0.13s\n",
      "Epoch [26/100], Loss: 0.0817, Time: 0.14s\n",
      "Epoch [27/100], Loss: 0.0721, Time: 0.14s\n",
      "Epoch [28/100], Loss: 0.0646, Time: 0.14s\n",
      "Epoch [29/100], Loss: 0.0585, Time: 0.15s\n",
      "Epoch [30/100], Loss: 0.0536, Time: 0.15s\n",
      "Epoch [31/100], Loss: 0.0497, Time: 0.16s\n",
      "Epoch [32/100], Loss: 0.0467, Time: 0.16s\n",
      "Epoch [33/100], Loss: 0.0447, Time: 0.16s\n",
      "Epoch [34/100], Loss: 0.0433, Time: 0.17s\n",
      "Epoch [35/100], Loss: 0.0421, Time: 0.17s\n",
      "Epoch [36/100], Loss: 0.0411, Time: 0.18s\n",
      "Epoch [37/100], Loss: 0.0400, Time: 0.18s\n",
      "Epoch [38/100], Loss: 0.0390, Time: 0.19s\n",
      "Epoch [39/100], Loss: 0.0382, Time: 0.19s\n",
      "Epoch [40/100], Loss: 0.0377, Time: 0.19s\n",
      "Epoch [41/100], Loss: 0.0373, Time: 0.20s\n",
      "Epoch [42/100], Loss: 0.0371, Time: 0.20s\n",
      "Epoch [43/100], Loss: 0.0368, Time: 0.21s\n",
      "Epoch [44/100], Loss: 0.0365, Time: 0.21s\n",
      "Epoch [45/100], Loss: 0.0362, Time: 0.22s\n",
      "Epoch [46/100], Loss: 0.0359, Time: 0.22s\n",
      "Epoch [47/100], Loss: 0.0357, Time: 0.23s\n",
      "Epoch [48/100], Loss: 0.0355, Time: 0.23s\n",
      "Epoch [49/100], Loss: 0.0354, Time: 0.24s\n",
      "Epoch [50/100], Loss: 0.0354, Time: 0.24s\n",
      "Epoch [51/100], Loss: 0.0353, Time: 0.25s\n",
      "Epoch [52/100], Loss: 0.0351, Time: 0.25s\n",
      "Epoch [53/100], Loss: 0.0350, Time: 0.26s\n",
      "Epoch [54/100], Loss: 0.0349, Time: 0.26s\n",
      "Epoch [55/100], Loss: 0.0348, Time: 0.27s\n",
      "Epoch [56/100], Loss: 0.0347, Time: 0.27s\n",
      "Epoch [57/100], Loss: 0.0347, Time: 0.27s\n",
      "Epoch [58/100], Loss: 0.0347, Time: 0.28s\n",
      "Epoch [59/100], Loss: 0.0346, Time: 0.28s\n",
      "Epoch [60/100], Loss: 0.0346, Time: 0.29s\n",
      "Epoch [61/100], Loss: 0.0345, Time: 0.29s\n",
      "Epoch [62/100], Loss: 0.0344, Time: 0.29s\n",
      "Epoch [63/100], Loss: 0.0344, Time: 0.30s\n",
      "Epoch [64/100], Loss: 0.0344, Time: 0.30s\n",
      "Epoch [65/100], Loss: 0.0344, Time: 0.31s\n",
      "Epoch [66/100], Loss: 0.0343, Time: 0.31s\n",
      "Epoch [67/100], Loss: 0.0343, Time: 0.31s\n",
      "Epoch [68/100], Loss: 0.0343, Time: 0.32s\n",
      "Epoch [69/100], Loss: 0.0342, Time: 0.32s\n",
      "Epoch [70/100], Loss: 0.0342, Time: 0.33s\n",
      "Epoch [71/100], Loss: 0.0342, Time: 0.33s\n",
      "Epoch [72/100], Loss: 0.0342, Time: 0.34s\n",
      "Epoch [73/100], Loss: 0.0342, Time: 0.34s\n",
      "Epoch [74/100], Loss: 0.0342, Time: 0.34s\n",
      "Epoch [75/100], Loss: 0.0341, Time: 0.35s\n",
      "Epoch [76/100], Loss: 0.0341, Time: 0.35s\n",
      "Epoch [77/100], Loss: 0.0341, Time: 0.36s\n",
      "Epoch [78/100], Loss: 0.0341, Time: 0.36s\n",
      "Epoch [79/100], Loss: 0.0341, Time: 0.37s\n",
      "Epoch [80/100], Loss: 0.0341, Time: 0.37s\n",
      "Epoch [81/100], Loss: 0.0340, Time: 0.38s\n",
      "Epoch [82/100], Loss: 0.0340, Time: 0.38s\n",
      "Epoch [83/100], Loss: 0.0340, Time: 0.38s\n",
      "Epoch [84/100], Loss: 0.0340, Time: 0.39s\n",
      "Epoch [85/100], Loss: 0.0340, Time: 0.39s\n",
      "Epoch [86/100], Loss: 0.0340, Time: 0.39s\n",
      "Epoch [87/100], Loss: 0.0340, Time: 0.40s\n",
      "Epoch [88/100], Loss: 0.0340, Time: 0.40s\n",
      "Epoch [89/100], Loss: 0.0339, Time: 0.41s\n",
      "Epoch [90/100], Loss: 0.0339, Time: 0.41s\n",
      "Epoch [91/100], Loss: 0.0339, Time: 0.41s\n",
      "Epoch [92/100], Loss: 0.0339, Time: 0.42s\n",
      "Epoch [93/100], Loss: 0.0339, Time: 0.42s\n",
      "Epoch [94/100], Loss: 0.0339, Time: 0.42s\n",
      "Epoch [95/100], Loss: 0.0339, Time: 0.43s\n",
      "Epoch [96/100], Loss: 0.0339, Time: 0.43s\n",
      "Epoch [97/100], Loss: 0.0339, Time: 0.44s\n",
      "Epoch [98/100], Loss: 0.0339, Time: 0.44s\n",
      "Epoch [99/100], Loss: 0.0339, Time: 0.44s\n",
      "Epoch [100/100], Loss: 0.0338, Time: 0.45s\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 50\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "seq_length = 10 # Reduced seq_length\n",
    "batch_size = 16 # Reduced batch_size\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data for training\n",
    "def get_batches(data, seq_length, batch_size):\n",
    "    n_batches = len(data) // (seq_length * batch_size)\n",
    "    valid_batches_x = []\n",
    "    valid_batches_y = []\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_index = i * batch_size * seq_length\n",
    "        end_index = start_index + batch_size * seq_length\n",
    "\n",
    "        batch_x = np.zeros((batch_size, seq_length), dtype=np.int64)\n",
    "        batch_y = np.zeros((batch_size, seq_length), dtype=np.int64)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            seq_start = start_index + j * seq_length\n",
    "            seq_end = seq_start + seq_length\n",
    "            if seq_end + 1 <= len(data):\n",
    "                batch_x[j, :] = data[seq_start:seq_end]\n",
    "                batch_y[j, :] = data[seq_start + 1:seq_end + 1]\n",
    "            else:\n",
    "                # If not enough data for a full sequence, skip this batch\n",
    "                batch_x = None\n",
    "                batch_y = None\n",
    "                break\n",
    "\n",
    "        if batch_x is not None and batch_y is not None:\n",
    "            valid_batches_x.append(torch.from_numpy(batch_x).to(device))\n",
    "            valid_batches_y.append(torch.from_numpy(batch_y).to(device))\n",
    "\n",
    "    if not valid_batches_x:\n",
    "        return torch.empty(0, batch_size, seq_length, dtype=torch.long).to(device), torch.empty(0, batch_size, seq_length, dtype=torch.long).to(device)\n",
    "\n",
    "    return torch.stack(valid_batches_x), torch.stack(valid_batches_y)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    x_batches, y_batches = get_batches(data, seq_length, batch_size)\n",
    "\n",
    "    if x_batches.size(0) == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], No batches available. Skipping epoch.\")\n",
    "        continue\n",
    "\n",
    "    num_batches = x_batches.size(0)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        inputs = x_batches[batch_idx]\n",
    "        targets = y_batches[batch_idx]\n",
    "        current_batch_size = inputs.size(0) # Get the actual batch size\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden(current_batch_size, device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (Teacher Forcing)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5) # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation (Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Greedy Search ---\n",
      "Starting with ' ':\n",
      " fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the l\n",
      "\n",
      "--- Temperature Sampling (T=0.5) ---\n",
      "Starting with 'y':\n",
      "y over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. \n",
      "\n",
      "--- Temperature Sampling (T=1.0) ---\n",
      "Starting with 'w':\n",
      "we the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy To. The quiick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The \n",
      "\n",
      "--- Temperature Sampling (T=1.5) ---\n",
      "Starting with 'f':\n",
      "fox jumps over the lazy dog. The quick brown fox jumps ocer the lazy born fox jumps over quick brown fox. jumps over the quick brown fox jumps over the lazy dog. The quick brown ffx fox jumps over the \n"
     ]
    }
   ],
   "source": [
    "def predict(model, start_char, predict_len=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated_text = [start_char]\n",
    "    input_eval = torch.tensor([[char_to_index[start_char]]], dtype=torch.long).to(device)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(predict_len):\n",
    "            outputs, hidden = model(input_eval, hidden)\n",
    "            # outputs shape: (1, 1, vocab_size)\n",
    "\n",
    "            # Apply temperature\n",
    "            output_logits = outputs[:, -1, :] / temperature\n",
    "            probabilities = nn.functional.softmax(output_logits, dim=-1)\n",
    "\n",
    "            # Sample the next character\n",
    "            predicted_index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            predicted_char = index_to_char[predicted_index]\n",
    "\n",
    "            generated_text.append(predicted_char)\n",
    "            input_eval = torch.tensor([[predicted_index]], dtype=torch.long).to(device)\n",
    "\n",
    "    return \"\".join(generated_text)\n",
    "\n",
    "# Greedy Search\n",
    "print(\"\\n--- Greedy Search ---\")\n",
    "start_char = random.choice(chars)\n",
    "generated_text_greedy = predict(model, start_char, predict_len=200, temperature=0.001) # Low temperature for greedy\n",
    "print(f\"Starting with '{start_char}':\\n{generated_text_greedy}\")\n",
    "\n",
    "# Temperature-based Sampling\n",
    "print(\"\\n--- Temperature Sampling (T=0.5) ---\")\n",
    "start_char = random.choice(chars)\n",
    "generated_text_temp_05 = predict(model, start_char, predict_len=200, temperature=0.5)\n",
    "print(f\"Starting with '{start_char}':\\n{generated_text_temp_05}\")\n",
    "\n",
    "print(\"\\n--- Temperature Sampling (T=1.0) ---\")\n",
    "start_char = random.choice(chars)\n",
    "generated_text_temp_10 = predict(model, start_char, predict_len=200, temperature=1.0)\n",
    "print(f\"Starting with '{start_char}':\\n{generated_text_temp_10}\")\n",
    "\n",
    "print(\"\\n--- Temperature Sampling (T=1.5) ---\")\n",
    "start_char = random.choice(chars)\n",
    "generated_text_temp_15 = predict(model, start_char, predict_len=200, temperature=1.5)\n",
    "print(f\"Starting with '{start_char}':\\n{generated_text_temp_15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
